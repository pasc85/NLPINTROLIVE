{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requiredlibraries.py with some corrections and additions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import operator\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import csv\n",
    "from collections import Counter\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import figure\n",
    "plt.rcParams.update({'font.size': 7})\n",
    "\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "from nltk.metrics import BigramAssocMeasures \n",
    "from nltk.collocations import *\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk_downloads = ['stopwords']\n",
    "###\n",
    "for s in nltk_downloads:\n",
    "    nltk.download(s)\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import models, corpora\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions copied from Saliha's course html\n",
    "\n",
    "def clean_up_text(text):\n",
    "    tokens = split_text_to_tokens(text)\n",
    "    tokens = clean_up_tokens(tokens)\n",
    "    processed_text = \" \".join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "def preprocess(pstr1):  # weaker cleaning\n",
    "     s=split_text_to_tokens(pstr1)\n",
    "     s=remove_non_alphabetic_tokens(s)\n",
    "     s=remove_punctuation_from_tokens(s)\n",
    "     s=set_tokens_to_lowercase(s)\n",
    "     return s\n",
    "\n",
    "def split_text_to_tokens(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def clean_up_tokens(tokens):\n",
    "    tokens = remove_punctuation_from_tokens(tokens)\n",
    "    tokens = remove_non_alphabetic_tokens(tokens)\n",
    "    tokens = set_tokens_to_lowercase(tokens)\n",
    "    tokens = remove_stopwords_from_tokens(tokens)\n",
    "    tokens = remove_small_words_from_tokens(tokens)\n",
    "    tokens = lemmatize_tokens(tokens)\n",
    "    tokens = remove_unimportant_words_from_tokens(tokens)\n",
    "    return tokens\n",
    "\n",
    "def remove_punctuation_from_tokens(tokens):\n",
    "    translation_table = str.maketrans({key: None for key in string.punctuation})\n",
    "    text_without_punctuations = []\n",
    "    for each_token in tokens:\n",
    "        text_without_punctuations.append(each_token.translate(translation_table))\n",
    "    return text_without_punctuations\n",
    "\n",
    "def remove_non_alphabetic_tokens(tokens):\n",
    "    alphabetic_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.isalpha():\n",
    "            alphabetic_tokens.append(token)\n",
    "    return alphabetic_tokens\n",
    "\n",
    "def set_tokens_to_lowercase(tokens):\n",
    "    lowercase_tokens = []\n",
    "    return [each_token.lower() for each_token in tokens]\n",
    "\n",
    "def remove_stopwords_from_tokens(tokens):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return [each_token for each_token in tokens if each_token not in stop_words]\n",
    "\n",
    "def remove_small_words_from_tokens(tokens):\n",
    "    return [each_token for each_token in tokens if len(each_token) > 2]\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    parts_of_speech = [wordnet.ADJ, wordnet.ADJ_SAT, wordnet.ADV, wordnet.NOUN, wordnet.VERB]\n",
    "    lemmatized_tokens = tokens\n",
    "###\n",
    "    for each_part_of_speech in parts_of_speech:\n",
    "        lemmatized_tokens = [wordnet_lemmatizer.lemmatize(each_token, pos=each_part_of_speech) for each_token in lemmatized_tokens]     \n",
    "    return lemmatized_tokens\n",
    "\n",
    "def remove_unimportant_words_from_tokens(tokens):\n",
    "    lemmatized_tokens = lemmatize_tokens(tokens)\n",
    "    tokens_with_part_of_speech_tags = nltk.pos_tag(lemmatized_tokens)  \n",
    "###\n",
    "    cleared_token_list = [each_token[0] for each_token in tokens_with_part_of_speech_tags if each_token[1] in [\"JJ\", \"JJR\", \"JJS\", \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"RB\", \"RBR\", \"RBS\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]]\n",
    "    # JJ (adjective), NN (noun), NNP (proper noun), RB (adverb), VB (verb) \n",
    "    return cleared_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the High Energy Physics Dataset \n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "hep_df = pd.read_pickle(\"./Hep_Dataset.pkl\")\n",
    "hep_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hep_df.Abstract = hep_df.Abstract.apply(clean_up_text)\n",
    "hep_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
