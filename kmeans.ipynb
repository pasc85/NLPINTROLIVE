{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Oct 22 11:48:27 2019\n",
    "\n",
    "@author: s-minhas\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import operator\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from wordcloud import WordCloud\n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "from nltk.metrics import BigramAssocMeasures \n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "plt.rcParams.update({'font.size': 7})\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "raw_data = pd.read_csv(\"SMSSpamCollection.csv\",  encoding='iso-8859-1') \n",
    "\n",
    "def split_text_to_tokens(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def remove_punctuation_from_tokens(tokens):\n",
    "    #punctuation = ['(', ')', '?', ':', ';', ',', '.', '!', '/', '\"', \"'\"] \n",
    "    punctuation='!?,.:;\"\\')(_-'\n",
    "    text_without_punctuations = []\n",
    "    for entity in tokens:\n",
    "        newstring = \"\"\n",
    "        for char in entity:\n",
    "            if(char not in punctuation):\n",
    "                  newstring+= char\n",
    "        text_without_punctuations.append(newstring)\n",
    "    return text_without_punctuations\n",
    "  \n",
    "\n",
    "def remove_non_alphabetic_tokens(tokens):\n",
    "    alphabetic_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.isalpha():\n",
    "            alphabetic_tokens.append(token)\n",
    "    return alphabetic_tokens\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords_from_tokens(tokens):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return [each_token for each_token in tokens if each_token not in stop_words]\n",
    "\n",
    "\n",
    "def set_tokens_to_lowercase(tokens):\n",
    "    \n",
    "    return [each_token.lower() for each_token in tokens]\n",
    "\n",
    "def preprocess(pstr1):\n",
    "    \n",
    "     s=split_text_to_tokens(pstr1)\n",
    "     s=remove_non_alphabetic_tokens(s)\n",
    "     s=remove_punctuation_from_tokens(s)\n",
    "     s=set_tokens_to_lowercase(s)\n",
    "     return s\n",
    "\n",
    "\n",
    "def getsampledata(pdf, psamp):\n",
    "    types = ['spam', 'ham']\n",
    "    allsamples = pd.DataFrame()\n",
    "    for i in types:\n",
    "        data1 = pdf[pdf.Email == i]\n",
    "        rows = np.random.choice(data1.index.values, psamp)\n",
    "        sampled_data = pdf.loc[rows] \n",
    "        allsamples = allsamples.append(sampled_data, ignore_index=True)\n",
    "\n",
    "    return allsamples\n",
    "\n",
    "\n",
    "samp_data = getsampledata (raw_data, 10)\n",
    "\n",
    "\n",
    "\n",
    "print (\"Sample data\", samp_data)\n",
    "\n",
    "alltext = \" \"   \n",
    "\n",
    "for index, row in samp_data.iterrows():\n",
    "           row['Description']=' '.join(preprocess(row['Description']))\n",
    "           alltext = row['Description'] + alltext\n",
    "    \n",
    "print (\"Sample data\", samp_data.head())\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "samp_data_vectorised = vectorizer.fit_transform(samp_data['Description'])\n",
    "\n",
    "\n",
    "tdm = pd.DataFrame(samp_data_vectorised.toarray(), columns = vectorizer.get_feature_names())\n",
    "print (tdm)\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=2)\n",
    "clusters = km.fit(tdm)\n",
    "#Show counts per cluster number\n",
    "print(\"Counts per Cluster\", np.unique(clusters.labels_, return_counts=True))\n",
    "\n",
    "#Check same number of documents returned\n",
    "print(\"Number of documents clustered\", np.unique(clusters.labels_, return_counts=True)[1].sum())\n",
    "\n",
    "#Show number of iterations of K-means\n",
    "print(\"number of iterations: {0}\".format(clusters.n_iter_))\n",
    "#add the cluster number to each input iati record\n",
    "samp_data['clusterresult']=clusters.labels_\n",
    "print (samp_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
