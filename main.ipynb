{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Sep 19 10:14:08 2019\n",
    "\n",
    "@author: s-minhas\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import operator\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from wordcloud import WordCloud\n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "from nltk.metrics import BigramAssocMeasures \n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "plt.rcParams.update({'font.size': 7})\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#wd = r'C:/IR Course/NLP_Intro_Course-master/spamham'\n",
    "\n",
    "\n",
    "def split_text_to_tokens(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def remove_punctuation_from_tokens(tokens):\n",
    "    #punctuation = ['(', ')', '?', ':', ';', ',', '.', '!', '/', '\"', \"'\"] \n",
    "    punctuation='!?,.:;\"\\')(_-'\n",
    "    text_without_punctuations = []\n",
    "    for entity in tokens:\n",
    "        newstring = \"\"\n",
    "        for char in entity:\n",
    "            if(char not in punctuation):\n",
    "                  newstring+= char\n",
    "        text_without_punctuations.append(newstring)\n",
    "    return text_without_punctuations\n",
    "  \n",
    "\n",
    "def remove_non_alphabetic_tokens(tokens):\n",
    "    alphabetic_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.isalpha():\n",
    "            alphabetic_tokens.append(token)\n",
    "    return alphabetic_tokens\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords_from_tokens(tokens):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return [each_token for each_token in tokens if each_token not in stop_words]\n",
    "\n",
    "\n",
    "def set_tokens_to_lowercase(tokens):\n",
    "    \n",
    "    return [each_token.lower() for each_token in tokens]\n",
    "\n",
    "def preprocess(pstr1):\n",
    "    \n",
    "     s=split_text_to_tokens(pstr1)\n",
    "     s=remove_non_alphabetic_tokens(s)\n",
    "     s=remove_punctuation_from_tokens(s)\n",
    "     s=set_tokens_to_lowercase(s)\n",
    "     return s\n",
    " \n",
    "##################\n",
    "\n",
    "raw_data = pd.read_csv(\"SMSSpamCollection.csv\",  encoding='iso-8859-1') \n",
    "\n",
    "raw_data[\"Email\"].value_counts().plot(kind = 'pie', explode = [0, 0.1], figsize = (6, 6), autopct = '%1.1f%%', shadow = True)\n",
    "plt.ylabel(\"Spam vs Ham\")\n",
    "plt.legend([\"Ham\", \"Spam\"])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def getsampledata(pdf, psamp):\n",
    "    types = ['spam', 'ham']\n",
    "    allsamples = pd.DataFrame()\n",
    "    for i in types:\n",
    "        data1 = pdf[pdf.Email == i]\n",
    "        rows = np.random.choice(data1.index.values, psamp)\n",
    "        sampled_data = pdf.loc[rows] \n",
    "        allsamples = allsamples.append(sampled_data, ignore_index=True)\n",
    "\n",
    "    return allsamples\n",
    "\n",
    "samp_data = getsampledata (raw_data, 10)\n",
    "\n",
    "def populatedictcorpus(data):\n",
    "    pdict1 = {} \n",
    "    textspam = \"\"\n",
    "    textham = \"\"\n",
    "    list_WtV_spam=[]\n",
    "    list_WtV_ham=[]\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "            if row['Email']=='spam':\n",
    "                                      #s= preprocess(row['Description'])\n",
    "                                      #textspam = textspam + \" \".join(s) \n",
    "                                      textspam = row['Description'] + \" \" +  textspam\n",
    "                                      list_WtV_spam.append(row['Description'].split(\" \")) \n",
    " \n",
    "            else:   \n",
    "                                      #s= preprocess(row['Description'])\n",
    "                                      #textham = textham + \" \".join(s) \n",
    "                                      textham = row['Description'] + \" \" +  textham\n",
    "                                      list_WtV_ham.append(row['Description'].split(\" \")) \n",
    "                                      \n",
    "                                      \n",
    "   \n",
    "    pdict1.update({'spam': textspam})\n",
    "    pdict1.update({'ham': textham})\n",
    "   \n",
    "    alldata = [pdict1,list_WtV_spam, list_WtV_ham ]\n",
    "   \n",
    "    return alldata\n",
    "   \n",
    "  \n",
    "def freqalltokens(palltext):\n",
    "        \n",
    "            dictcounts = {}\n",
    "            palltext = palltext.split(\" \")\n",
    "            for token in palltext:\n",
    "                if token in dictcounts:\n",
    "                    dictcounts[token] = dictcounts[token] + 1\n",
    "                else:\n",
    "                     dictcounts[token] =  1\n",
    "            sorted_val = sorted(dictcounts.items(), key=operator.itemgetter(1), reverse=True)         \n",
    "            return sorted_val          \n",
    "\n",
    "\n",
    "def plotall(px, py):\n",
    "\n",
    "    plt.xticks(fontsize=6, rotation=90)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.plot(px, py)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def percentoftotal (px, py, psum):\n",
    "    pfreqdict = {}\n",
    "    for i in range (10):\n",
    "         pfreqlist = []\n",
    "         t= py[i]/psum *100\n",
    "         pfreqlist.append(py[i])\n",
    "         pfreqlist.append(round(t, 3))\n",
    "         t = py[i]/psum * 1000\n",
    "         pfreqlist.append(round(t,3))\n",
    "         pfreqdict[px[i]] = pfreqlist\n",
    "    return pfreqdict\n",
    "\n",
    "####lexical diversity\n",
    "\n",
    "def lexical_diversity(text):\n",
    "  \n",
    "    info = []\n",
    "    info.append(len(text))\n",
    "    info.append(len(set(text))) \n",
    "    info.append(len(set(text))/len(text))\n",
    "    return info\n",
    "\n",
    "         \n",
    "count_spamham = []\n",
    "sum_tokens=0\n",
    "alltext = \"\"\n",
    "complete_list = populatedictcorpus(samp_data)\n",
    "dict1 = complete_list[0]\n",
    "for key in dict1:\n",
    "    count_spamham.append([key, len(dict1[key])])\n",
    "    sum_tokens=len(dict1[key]) + sum_tokens\n",
    "    alltext = alltext + dict1[key]\n",
    "\n",
    "a=freqalltokens(alltext)\n",
    "\n",
    "#x, y = zip(*freqalltokens(alltext))\n",
    "\n",
    "token = []\n",
    "count = []\n",
    "for item in a:\n",
    "    token.append(item[0])\n",
    "    count.append(item[1])\n",
    "    \n",
    " \n",
    "import csv\n",
    "\n",
    "with open(\"datafreq.csv\", mode='w', newline='', encoding='iso-8859-1') as datafreq:\n",
    "    datafreq = csv.writer(datafreq, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    datafreq.writerow(token)\n",
    "    datafreq.writerow(count)\n",
    "\n",
    " \n",
    "    \n",
    "    \n",
    "plotall(token, count)\n",
    "\n",
    "\n",
    "for itemnum in range (len(count_spamham)):\n",
    "    print (\"Number of tokens in:\", count_spamham[itemnum][0], count_spamham[itemnum][1])\n",
    "print (\"Number of tokens in text:\", sum_tokens)\n",
    "\n",
    "\n",
    "\n",
    "freqdict= percentoftotal (token, count, sum_tokens)\n",
    "\n",
    "##plot\n",
    "tokens = tuple(freqdict.keys())\n",
    "values = freqdict.values()\n",
    "total, percent,normalised = zip(*values)\n",
    "\n",
    "\n",
    "plotall(tokens, normalised)\n",
    "\n",
    "\n",
    "lingstats = lexical_diversity(dict1['spam'] + \" \" + dict1['ham'])\n",
    "print (\"Total tokens:\", lingstats[0], \"Total Unique Words:\", lingstats[1], \"Type/Token Ratio:\", round(lingstats[2], 6))\n",
    "\n",
    "\n",
    "#Spam Word cloud\n",
    "\n",
    "def words_to_cloud (pstr):\n",
    "    wordcloud = WordCloud().generate(pstr)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "words_to_cloud (\" \".join(dict1['spam'].split(\" \")))\n",
    "\n",
    "#dispersion\n",
    "#spam_text_tokens = nltk.word_tokenize(dict1['spam']) #tokenize\n",
    "spam_text_tokens = nltk.word_tokenize(dict1['spam']) #tokenize\n",
    "spam_text_object = nltk.Text(spam_text_tokens) #turning it into nltk.Text object to be able to use .condordance, .similar etc\n",
    "spam_text_object.dispersion_plot([\"call\", \"service\", \"text\"])\n",
    "\n",
    "###################\n",
    "   \n",
    "# get concordance\n",
    "\n",
    "allspamtokens = nltk.word_tokenize(dict1['spam']) #tokenize\n",
    "spamtoken_object = nltk.Text(allspamtokens) #turning it into nltk.Text object to be able to use .condordance, .similar etc\n",
    "spamtoken_object.concordance('call')\n",
    "\n",
    "################\n",
    "\n",
    "##Get bigrams\n",
    "\n",
    "#bigrm = nltk.bigrams(dict1['ham'].split(\" \"))\n",
    "#biagram_collocation = BigramCollocationFinder.from_words(dict1['ham'].split(\" \")) \n",
    "#biagram_collocation.nbest(BigramAssocMeasures.likelihood_ratio, 15) \n",
    "\n",
    "\n",
    "def generate_collocations(tokens):\n",
    "    '''\n",
    "    Given list of tokens, return collocations.\n",
    "    '''\n",
    "\n",
    "    ignored_words = nltk.corpus.stopwords.words('english')\n",
    "    bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "    bigramFinder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "    bigram_freq = bigramFinder.ngram_fd.items()\n",
    "    bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "   \n",
    "    return bigramFreqTable\n",
    "\n",
    "print (generate_collocations(dict1['spam'].split()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###plot embeddings\n",
    "complete_list = populatedictcorpus(raw_data)\n",
    "#spam\n",
    "model = Word2Vec(complete_list[1], min_count=20,size=50,workers=4)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "# access vector for one word\n",
    "print(model['win'])\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)\n",
    "\n",
    "#plot with PCA\n",
    "\n",
    "#ham\n",
    "\n",
    "model2 = Word2Vec(complete_list[2], min_count=20,size=50,workers=4)\n",
    "# summarize the loaded model\n",
    "print(model2)\n",
    "# summarize vocabulary\n",
    "words2 = list(model2.wv.vocab)\n",
    "print(words2)\n",
    "# access vector for one word\n",
    "print(model2['guy'])\n",
    "# save model\n",
    "model2.save('model2.bin')\n",
    "# load modelhttp://localhost:8888/notebooks/solution%20to%20ex%203.ipynb#\n",
    "new_model2 = Word2Vec.load('model2.bin')\n",
    "print(new_model2)\n",
    "\n",
    "# dimensionality reduction \n",
    "X = model[model.wv.vocab]\n",
    "X2 = model2[model2.wv.vocab]\n",
    "\n",
    "\n",
    "pca1 = PCA(n_components=2)\n",
    "result = pca1.fit_transform(X)\n",
    "\n",
    "pca2 = PCA(n_components=2)\n",
    "result2 = pca2.fit_transform(X2)\n",
    "\n",
    "# Create plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(result[:, 0], result[:, 1], c=\"red\",s=5,label=\"spam\")\n",
    "ax.scatter(result2[:, 0], result2[:, 1], c=\"blue\",s=5,label=\"ham\")\n",
    "plt.xlim(-0.50, 1.25) \n",
    "plt.ylim(-0.04, 0.04)\n",
    "plt.gcf().set_size_inches((10, 10))   \n",
    "\n",
    "\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "\tplt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "\n",
    "\n",
    "words2 = list(model2.wv.vocab)\n",
    "for i, word2 in enumerate(words2):\n",
    "\tplt.annotate(word2, xy=(result2[i, 0], result2[i, 1]))\n",
    "\n",
    "\n",
    "plt.title('Spam Ham Embeddings')\n",
    "plt.legend(loc=2)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##separate\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4), sharey=True, dpi=120)\n",
    "\n",
    "# Plot\n",
    "ax1.scatter(result[:, 0], result[:, 1], c=\"red\",label=\"spam\", s= 5)\n",
    "\n",
    "ax2.scatter(result2[:, 0], result2[:, 1], c=\"blue\",label=\"ham\", s= 5)\n",
    "\n",
    "# Title, X and Y labels, X and Y Lim\n",
    "ax1.set_title('Spam Embeddings'); ax2.set_title('Ham Embeddings')\n",
    "ax1.set_xlabel('X');  ax2.set_xlabel('X')  # x label\n",
    "ax1.set_ylabel('Y');  ax2.set_ylabel('Y')  # y label\n",
    "ax1.set_xlim(-0.50, 1.25) ;  ax2.set_xlim(-0.50, 1.25)   # x axis limits\n",
    "ax1.set_ylim(-0.04, 0.04);  ax2.set_ylim(-0.04, 0.04)  # y axis limits\n",
    "\n",
    "\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "\tax1.annotate(word, xy=(result[i, 0], result[i, 1]), fontsize=5)\n",
    "\n",
    "\n",
    "words2 = list(model2.wv.vocab)\n",
    "for i, word2 in enumerate(words2):\n",
    "\tax2.annotate(word2, xy=(result2[i, 0], result2[i, 1]), fontsize=5)\n",
    "\n",
    "\n",
    "ax1.legend(loc=2)\n",
    "ax2.legend(loc=5)\n",
    "\n",
    "\n",
    "# ax2.yaxis.set_ticks_position('none') \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
